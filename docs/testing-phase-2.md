# Testing Phase 2: Multi-Skill Orchestration

This guide walks you through testing all Phase 2 features of the Conversational Skill Builder.

## Table of Contents
1. [Sequential Orchestration](#1-sequential-orchestration)
2. [APScheduler Integration](#2-apscheduler-integration)
3. [Public Skill Library](#3-public-skill-library)
4. [Multi-Skill Conversation](#4-multi-skill-conversation)
5. [Skill Versioning](#5-skill-versioning)
6. [Observability & Monitoring](#6-observability--monitoring)
7. [End-to-End Integration](#7-end-to-end-integration)

---

## 1. Sequential Orchestration

Test multi-skill execution with data flow between skills.

### Unit Tests
```bash
# Run all orchestration tests
pytest tests/execution/orchestration/ -v

# Expected: 17 tests passing
```

### Interactive Demo
```python
# File: examples/test_sequential_orchestration.py
import asyncio
from omniforge.builder.models import AgentConfig, SkillReference
from omniforge.execution.orchestration.engine import OrchestrationEngine
from omniforge.execution.orchestration.strategies import ErrorStrategy

async def demo_sequential_execution():
    """Demonstrate sequential multi-skill execution."""

    # Create agent with 3 skills in sequence
    agent = AgentConfig(
        tenant_id="demo-tenant",
        name="Multi-Step Report Generator",
        description="Fetch from Notion, process data, post to Slack",
        skills=[
            SkillReference(
                skill_id="fetch-notion-data",
                name="Fetch Notion Data",
                order=1,
                error_strategy=ErrorStrategy.STOP_ON_ERROR
            ),
            SkillReference(
                skill_id="process-data",
                name="Process Data",
                order=2,
                error_strategy=ErrorStrategy.RETRY_ON_ERROR,
                max_retries=3
            ),
            SkillReference(
                skill_id="post-to-slack",
                name="Post to Slack",
                order=3,
                error_strategy=ErrorStrategy.SKIP_ON_ERROR
            ),
        ],
        created_by="demo-user",
    )

    # Create orchestration engine
    engine = OrchestrationEngine()

    # Execute agent
    result = await engine.execute_agent(
        agent=agent,
        input_data={"project_id": "proj-123"}
    )

    print(f"Execution Status: {result.status}")
    print(f"Total Duration: {result.duration_ms}ms")
    print(f"\nSkill Results:")
    for i, skill_result in enumerate(result.skill_results, 1):
        print(f"  {i}. {skill_result.skill_id}: {skill_result.status}")
        print(f"     Duration: {skill_result.duration_ms}ms")
        if skill_result.output:
            print(f"     Output: {skill_result.output}")

if __name__ == "__main__":
    asyncio.run(demo_sequential_execution())
```

### Key Features to Test
- ‚úÖ Skills execute in order (1, 2, 3...)
- ‚úÖ Output from skill N flows to skill N+1 as input
- ‚úÖ STOP_ON_ERROR halts immediately on failure
- ‚úÖ SKIP_ON_ERROR continues execution
- ‚úÖ RETRY_ON_ERROR retries up to max_retries times
- ‚úÖ Per-skill timing and status tracking

---

## 2. APScheduler Integration

Test scheduled agent execution with cron expressions.

### Unit Tests
```bash
# Run scheduler tests
pytest tests/execution/test_scheduler.py -v

# Expected: 18 tests passing
```

### Interactive Demo
```python
# File: examples/test_scheduler.py
import asyncio
from datetime import datetime
from omniforge.execution.scheduler import AgentScheduler, ScheduleConfig

async def demo_scheduler():
    """Demonstrate scheduled agent execution."""

    # Create scheduler
    scheduler = AgentScheduler()
    scheduler.start()

    # Add schedule for daily execution at 8am
    config = ScheduleConfig(
        agent_id="report-agent-123",
        tenant_id="demo-tenant",
        cron_expression="0 8 * * *",  # Daily at 8am
        timezone="America/New_York",
        enabled=True
    )

    success = await scheduler.add_schedule(config)
    print(f"Schedule added: {success}")

    # List all schedules
    schedules = scheduler.list_schedules()
    for schedule in schedules:
        next_run = scheduler.get_next_run_time(schedule.agent_id)
        print(f"\nAgent: {schedule.agent_id}")
        print(f"  Cron: {schedule.cron_expression}")
        print(f"  Timezone: {schedule.timezone}")
        print(f"  Next Run: {next_run}")

    # Shutdown gracefully
    scheduler.shutdown()

if __name__ == "__main__":
    asyncio.run(demo_scheduler())
```

### Test with FastAPI
```bash
# Start the FastAPI app
uvicorn omniforge.api.app:app --reload

# The scheduler will auto-start and load scheduled agents from database
# Check logs for: "Scheduler started"
```

### Key Features to Test
- ‚úÖ Add/update/remove schedules
- ‚úÖ Cron expression parsing
- ‚úÖ Timezone-aware scheduling
- ‚úÖ Auto-reload schedules on startup
- ‚úÖ Missed execution logging
- ‚úÖ Graceful shutdown

---

## 3. Public Skill Library

Test skill discovery, search, and usage tracking.

### Unit Tests
```bash
# Run public skill library tests
pytest tests/builder/test_public_skill_repository.py tests/builder/discovery/ -v

# Expected: 32 tests passing
```

### Interactive Demo
```python
# File: examples/test_public_skills.py
import asyncio
from omniforge.builder.models.public_skill import PublicSkill, PublicSkillStatus
from omniforge.builder.repository import PublicSkillRepository
from omniforge.builder.discovery.service import SkillDiscoveryService
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession

async def demo_public_skills():
    """Demonstrate public skill library features."""

    # Setup database
    engine = create_async_engine("sqlite+aiosqlite:///:memory:")
    async with engine.begin() as conn:
        from omniforge.builder.models.orm import Base
        await conn.run_sync(Base.metadata.create_all)

    session_maker = async_sessionmaker(engine, class_=AsyncSession)

    async with session_maker() as session:
        repo = PublicSkillRepository(session)

        # Create sample public skills
        skills = [
            PublicSkill(
                name="notion-fetch-pages",
                version="1.0.0",
                description="Fetch pages from Notion database",
                content="# Notion Fetch Pages\n...",
                author="community",
                integration_type="notion",
                tags=["notion", "fetch", "database"],
                status=PublicSkillStatus.APPROVED,
                usage_count=150,
                rating_avg=4.5,
                rating_count=20
            ),
            PublicSkill(
                name="slack-post-message",
                version="2.1.0",
                description="Post formatted messages to Slack channels",
                content="# Slack Post Message\n...",
                author="community",
                integration_type="slack",
                tags=["slack", "post", "message"],
                status=PublicSkillStatus.APPROVED,
                usage_count=300,
                rating_avg=4.8,
                rating_count=50
            ),
        ]

        for skill in skills:
            await repo.create(skill)
        await session.commit()

        # Search skills
        print("=== Search Results: 'notion' ===")
        results = await repo.search(keyword="notion", limit=5)
        for skill in results:
            print(f"  {skill.name} v{skill.version} - {skill.usage_count} uses")

        # Get by integration
        print("\n=== Slack Skills ===")
        slack_skills = await repo.get_by_integration("slack")
        for skill in slack_skills:
            print(f"  {skill.name} - ‚≠ê {skill.rating_avg}/5.0")

        # Discover skills based on context
        print("\n=== Skill Discovery ===")
        discovery = SkillDiscoveryService(repo)
        recommendations = await discovery.discover_by_context(
            description="I need to fetch data from Notion and post to Slack",
            integration_filter=None,
            limit=3
        )

        for rec in recommendations:
            print(f"\n  {rec.skill.name} v{rec.skill.version}")
            print(f"  Relevance: {rec.relevance_score:.2f}")
            print(f"  Reason: {rec.reason}")

        # Increment usage count
        await repo.increment_usage_count("notion-fetch-pages", "1.0.0")
        await session.commit()

        updated = await repo.get_by_name("notion-fetch-pages", "1.0.0")
        print(f"\n=== Updated Usage ===")
        print(f"  {updated.name}: {updated.usage_count} uses")

if __name__ == "__main__":
    asyncio.run(demo_public_skills())
```

### Test via API
```bash
# Start the API
uvicorn omniforge.api.app:app --reload

# Search skills
curl http://localhost:8000/api/v1/skills/?keyword=notion

# Discover skills
curl -X POST http://localhost:8000/api/v1/skills/discover \
  -H "Content-Type: application/json" \
  -d '{"description": "fetch data and post to slack", "limit": 5}'

# Get popular skills
curl http://localhost:8000/api/v1/skills/popular/top?limit=10
```

### Key Features to Test
- ‚úÖ Search by keyword, tags, integration
- ‚úÖ Popularity-based ranking
- ‚úÖ Relevance scoring algorithm
- ‚úÖ Usage tracking
- ‚úÖ Version management integration
- ‚úÖ Approval workflow (pending/approved/rejected)

---

## 4. Multi-Skill Conversation

Test conversational creation of multi-skill agents.

### Unit Tests
```bash
# Run multi-skill conversation tests
pytest tests/builder/generation/ tests/builder/conversation/test_multi_skill.py -v

# Expected: 32 tests passing
```

### Interactive Demo
```python
# File: examples/test_multi_skill_conversation.py
import asyncio
from omniforge.builder.conversation.manager import ConversationManager

async def demo_multi_skill_conversation():
    """Demonstrate multi-skill agent creation via conversation."""

    manager = ConversationManager()
    conversation_id = "demo-multi-skill"

    # Start conversation
    context = manager.start_conversation(
        conversation_id=conversation_id,
        tenant_id="demo-tenant",
        user_id="demo-user"
    )

    print("ü§ñ Assistant: What would you like to automate?\n")

    # User describes multi-skill need
    user_input = "Fetch weekly reports from Notion and post them to Slack every Monday"
    print(f"üë§ You: {user_input}\n")

    context, response = manager.process_user_input(conversation_id, user_input)
    print(f"ü§ñ Assistant: {response}\n")
    print(f"   [State: {context.state.value}]\n")

    # Continue conversation
    interactions = [
        ("Notion and Slack", "Which integrations?"),
        ("Yes", "Does the suggested flow look good?"),
        ("Yes, ready", "Ready to connect integrations?"),
        ("Generate reports every Monday at 8am", "What should the agent do?"),
        ("Yes", "Confirm agent creation?"),
    ]

    for user_msg, prompt_hint in interactions:
        print(f"üë§ You: {user_msg}\n")
        context, response = manager.process_user_input(conversation_id, user_msg)
        print(f"ü§ñ Assistant: {response}\n")
        print(f"   [State: {context.state.value}]\n")

        if context.state.value == "complete":
            break

    # Show final agent configuration
    if context.agent_config:
        print("=" * 60)
        print("‚úÖ AGENT CREATED!\n")
        print(f"Name: {context.agent_config.name}")
        print(f"Description: {context.agent_config.description}")
        print(f"Skills: {len(context.agent_config.skills)}")
        print("\nSkill Flow:")
        for skill in sorted(context.agent_config.skills, key=lambda s: s.order):
            print(f"  {skill.order}. {skill.name}")
        print("=" * 60)

if __name__ == "__main__":
    asyncio.run(demo_multi_skill_conversation())
```

### Key Features to Test
- ‚úÖ Multi-skill detection from description
- ‚úÖ Plain language flow explanation
- ‚úÖ Public skill suggestions
- ‚úÖ Skill ordering captured
- ‚úÖ Mix public and custom skills
- ‚úÖ User can modify composition
- ‚úÖ Integration detection

---

## 5. Skill Versioning

Test semantic versioning for public skills.

### Unit Tests
```bash
# Run versioning tests
pytest tests/builder/versioning/ tests/builder/test_public_skill_versioning.py -v

# Expected: 35 tests passing
```

### Interactive Demo
```python
# File: examples/test_skill_versioning.py
import asyncio
from omniforge.builder.models.public_skill import PublicSkill, PublicSkillStatus
from omniforge.builder.repository import PublicSkillRepository
from omniforge.builder.versioning.resolver import VersionResolver, SemanticVersion
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession

async def demo_versioning():
    """Demonstrate skill version management."""

    # Setup database
    engine = create_async_engine("sqlite+aiosqlite:///:memory:")
    async with engine.begin() as conn:
        from omniforge.builder.models.orm import Base
        await conn.run_sync(Base.metadata.create_all)

    session_maker = async_sessionmaker(engine, class_=AsyncSession)

    async with session_maker() as session:
        repo = PublicSkillRepository(session)

        # Create multiple versions of the same skill
        versions = ["1.0.0", "1.1.0", "1.2.0", "2.0.0"]
        for version in versions:
            skill = PublicSkill(
                name="data-processor",
                version=version,
                description=f"Data processing skill v{version}",
                content=f"# Data Processor v{version}\n...",
                author="community",
                tags=["processing"],
                status=PublicSkillStatus.APPROVED
            )
            await repo.create(skill)
        await session.commit()

        # Get all versions
        print("=== All Versions ===")
        all_versions = await repo.get_versions("data-processor")
        for skill in all_versions:
            print(f"  v{skill.version}")

        # Get latest version (version=None)
        print("\n=== Latest Version ===")
        latest = await repo.get_by_name("data-processor", version=None)
        print(f"  v{latest.version}")

        # Get specific version
        print("\n=== Specific Version ===")
        specific = await repo.get_by_name("data-processor", version="1.1.0")
        print(f"  v{specific.version}")

        # Version compatibility checking
        print("\n=== Version Compatibility ===")
        current = SemanticVersion.parse("1.2.0")
        target = SemanticVersion.parse("2.0.0")
        compatibility = VersionResolver.check_compatibility(current, target)
        print(f"  1.2.0 -> 2.0.0: {compatibility.value}")

        warning = VersionResolver.get_version_warning(current, target)
        if warning:
            print(f"  ‚ö†Ô∏è  {warning}")

        # Check for newer versions
        print("\n=== Newer Versions Available ===")
        current_version = SemanticVersion.parse("1.1.0")
        available_versions = [SemanticVersion.parse(v) for v in versions]
        has_newer = VersionResolver.has_newer_version(current_version, available_versions)
        print(f"  Newer than 1.1.0 available: {has_newer}")

        if has_newer:
            latest_version = VersionResolver.resolve_latest(available_versions)
            print(f"  Latest available: v{latest_version}")

if __name__ == "__main__":
    asyncio.run(demo_versioning())
```

### Key Features to Test
- ‚úÖ Multiple versions per skill
- ‚úÖ Semantic version parsing (MAJOR.MINOR.PATCH)
- ‚úÖ Version comparison operators
- ‚úÖ Latest version resolution
- ‚úÖ Version pinning in SkillReference
- ‚úÖ Compatibility checking
- ‚úÖ Breaking change warnings

---

## 6. Observability & Monitoring

Test metrics, logging, and tracing.

### Unit Tests
```bash
# Run observability tests
pytest tests/observability/ tests/api/test_health.py -v

# Expected: 28 tests passing
```

### Test Metrics Endpoint
```bash
# Start the API
uvicorn omniforge.api.app:app --reload

# Check metrics endpoint
curl http://localhost:8000/metrics

# Expected output (Prometheus format):
# agent_executions_total{status="success",agent_id="..."} 5
# skill_execution_duration_seconds_bucket{skill_id="...",le="1.0"} 10
# http_requests_total{method="GET",path="/health"} 20
```

### Test Health Check
```bash
# Check health endpoint
curl http://localhost:8000/health

# Expected output (200 OK):
{
  "status": "healthy",
  "timestamp": "2026-01-26T...",
  "components": {
    "database": {
      "status": "healthy",
      "message": "Database connection successful"
    },
    "scheduler": {
      "status": "healthy",
      "message": "Scheduler is running"
    },
    "llm": {
      "status": "healthy",
      "message": "LLM connectivity verified"
    }
  }
}
```

### Test Structured Logging
```python
# File: examples/test_logging.py
import structlog
from omniforge.observability.logging import setup_logging, get_logger

# Setup logging
setup_logging(json_logs=True, log_level="INFO")

# Get logger
logger = get_logger(__name__)

# Log with correlation ID
logger.info(
    "agent_execution_started",
    agent_id="agent-123",
    tenant_id="tenant-456",
    correlation_id="req-789"
)

# Expected output (JSON):
# {"event": "agent_execution_started", "agent_id": "agent-123", ...}
```

### Test Execution Tracing
```python
# File: examples/test_tracing.py
from omniforge.observability.tracing import get_execution_tracer

tracer = get_execution_tracer()

# Start trace
trace = tracer.start_trace(
    execution_id="exec-123",
    agent_id="agent-456",
    correlation_id="req-789"
)

# Add skill traces
trace.start_skill("skill-1", {"input": "data"})
# ... skill execution ...
trace.complete_skill("skill-1", "success", {"output": "result"})

# Get trace summary
summary = trace.to_dict()
print(f"Total duration: {summary['total_duration_ms']}ms")
print(f"Skills executed: {len(summary['skills'])}")
```

### Key Features to Test
- ‚úÖ Correlation IDs in all logs
- ‚úÖ Prometheus metrics collection
- ‚úÖ Health check with component status
- ‚úÖ JSON structured logs
- ‚úÖ Per-skill execution tracing
- ‚úÖ HTTP request metrics
- ‚úÖ Middleware correlation ID propagation

---

## 7. End-to-End Integration

Test all Phase 2 features working together.

### Complete Workflow Test
```python
# File: examples/test_phase2_e2e.py
import asyncio
from datetime import datetime
from omniforge.builder.conversation.manager import ConversationManager
from omniforge.builder.repository import PublicSkillRepository, AgentConfigRepository
from omniforge.execution.orchestration.engine import OrchestrationEngine
from omniforge.execution.scheduler import AgentScheduler, ScheduleConfig
from omniforge.observability.logging import get_logger
from omniforge.observability.metrics import get_metrics_collector
from omniforge.observability.tracing import get_execution_tracer
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession

async def test_complete_workflow():
    """Test complete Phase 2 workflow end-to-end."""

    logger = get_logger(__name__)
    metrics = get_metrics_collector()
    tracer = get_execution_tracer()

    logger.info("starting_phase2_e2e_test", correlation_id="test-001")

    # Setup database
    engine = create_async_engine("sqlite+aiosqlite:///:memory:")
    async with engine.begin() as conn:
        from omniforge.builder.models.orm import Base
        await conn.run_sync(Base.metadata.create_all)

    session_maker = async_sessionmaker(engine, class_=AsyncSession)

    async with session_maker() as session:
        # Step 1: Create conversation for multi-skill agent
        print("\n=== Step 1: Multi-Skill Conversation ===")
        manager = ConversationManager()
        context = manager.start_conversation("test-conv", "tenant-1", "user-1")

        context, response = manager.process_user_input(
            "test-conv",
            "Fetch Notion data and post to Slack every Monday"
        )
        print(f"Agent detected: {len(context.skill_needs.skills) if context.skill_needs else 0} skills")

        # Step 2: Discover public skills
        print("\n=== Step 2: Public Skill Discovery ===")
        skill_repo = PublicSkillRepository(session)
        # (Would populate with actual public skills in real scenario)

        # Step 3: Create agent with versioned skills
        print("\n=== Step 3: Create Agent ===")
        agent_repo = AgentConfigRepository(session)
        # (Agent created through conversation in real scenario)

        # Step 4: Schedule agent execution
        print("\n=== Step 4: Schedule Agent ===")
        scheduler = AgentScheduler()
        scheduler.start()

        schedule = ScheduleConfig(
            agent_id="agent-123",
            tenant_id="tenant-1",
            cron_expression="0 8 * * 1",  # Monday at 8am
            timezone="UTC",
            enabled=True
        )
        await scheduler.add_schedule(schedule)
        print(f"Schedule added: {scheduler.get_next_run_time('agent-123')}")

        # Step 5: Execute agent with orchestration
        print("\n=== Step 5: Execute Agent ===")
        engine = OrchestrationEngine()

        # Start trace
        trace = tracer.start_trace("exec-001", "agent-123", "test-001")

        # (Would execute real agent in actual scenario)
        # result = await engine.execute_agent(agent, input_data)

        # Record metrics
        metrics.record_agent_execution("agent-123", "success")
        metrics.record_skill_execution("skill-1", "Fetch Data", 250, "success")
        metrics.record_skill_execution("skill-2", "Post Slack", 150, "success")

        # Step 6: Check observability
        print("\n=== Step 6: Observability ===")
        trace_summary = trace.to_dict()
        print(f"Trace ID: {trace_summary['execution_id']}")
        print(f"Correlation ID: {trace_summary['correlation_id']}")

        # Export metrics
        metrics_output = metrics.generate_metrics()
        print(f"Metrics collected: {len(metrics_output)} bytes")

        scheduler.shutdown()

    print("\n‚úÖ Phase 2 End-to-End Test Complete!")
    print("\nFeatures Verified:")
    print("  ‚úì Multi-skill conversation")
    print("  ‚úì Public skill discovery")
    print("  ‚úì Skill versioning")
    print("  ‚úì Agent scheduling")
    print("  ‚úì Sequential orchestration")
    print("  ‚úì Observability (metrics, logs, traces)")

if __name__ == "__main__":
    asyncio.run(test_complete_workflow())
```

### Run Complete Test Suite
```bash
# Run all Phase 2 tests
pytest tests/execution/ \
       tests/builder/versioning/ \
       tests/builder/discovery/ \
       tests/builder/generation/ \
       tests/builder/conversation/test_multi_skill.py \
       tests/observability/ \
       -v

# Expected: 132 tests passing
```

---

## Summary Checklist

Use this checklist to verify all Phase 2 features are working:

### Sequential Orchestration
- [ ] Multi-skill agents execute skills in order
- [ ] Data flows between skills correctly
- [ ] Error strategies work (STOP, SKIP, RETRY)
- [ ] Per-skill timing tracked
- [ ] All 17 tests pass

### APScheduler Integration
- [ ] Schedules can be added/updated/removed
- [ ] Cron expressions parse correctly
- [ ] Timezone-aware scheduling works
- [ ] Schedules persist across restarts
- [ ] All 18 tests pass

### Public Skill Library
- [ ] Skills can be searched and discovered
- [ ] Relevance scoring works correctly
- [ ] Usage tracking increments
- [ ] Versions are handled properly
- [ ] All 32 tests pass

### Multi-Skill Conversation
- [ ] Multi-skill needs detected from description
- [ ] Plain language explanations work
- [ ] Public skills suggested during conversation
- [ ] Skill ordering captured correctly
- [ ] All 32 tests pass

### Skill Versioning
- [ ] Multiple versions can be stored
- [ ] Latest version resolution works
- [ ] Version pinning in SkillReference works
- [ ] Compatibility checking works
- [ ] All 35 tests pass

### Observability
- [ ] Metrics endpoint returns Prometheus format
- [ ] Health check shows component status
- [ ] Correlation IDs in all logs
- [ ] Execution tracing works
- [ ] All 28 tests pass

### Integration
- [ ] All features work together
- [ ] No integration issues
- [ ] Performance acceptable
- [ ] All 132 Phase 2 tests pass

---

## Troubleshooting

### Issue: Import errors
**Solution**: Install all dependencies
```bash
pip install -e ".[dev]"
```

### Issue: Database errors
**Solution**: Ensure SQLite foreign keys enabled
```python
await conn.execute(text("PRAGMA foreign_keys=ON"))
```

### Issue: Scheduler not starting
**Solution**: Check FastAPI lifespan integration
```python
# In app.py, ensure lifespan context manager is used
app = FastAPI(lifespan=lifespan)
```

### Issue: Metrics not collected
**Solution**: Ensure metrics collector is imported
```python
from omniforge.observability.metrics import get_metrics_collector
metrics = get_metrics_collector()
```

---

## Next Steps

After verifying Phase 2 features:
1. Run performance tests to ensure < 3s response times
2. Test with real integrations (Notion, Slack)
3. Load test with multiple concurrent agents
4. Review logs and metrics in production-like environment
5. Proceed to Phase 3: Advanced Features & B2B2C

For questions or issues, refer to:
- Phase 2 implementation: `specs/tasks/conversational-skill-builder/phase-2-orchestration/`
- API documentation: `docs/observability.md`
- Test examples: `examples/` directory
